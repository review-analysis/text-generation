{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n",
    "import numpy as np\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "from keras.models import load_model\n",
    "\n",
    "from multiplicative_lstm import MultiplicativeLSTM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters: change to experiment different configurations\n",
    "SEQUENCE_LEN = 15\n",
    "MIN_WORD_FREQUENCY = 500\n",
    "STEP = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n",
    "    # shuffle at unison\n",
    "    print('Shuffling sentences')\n",
    "\n",
    "    tmp_sentences = []\n",
    "    tmp_next_word = []\n",
    "    for i in np.random.permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_word.append(next_original[i])\n",
    "\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
    "\n",
    "    print(\"Size of training set = %d\" % len(x_train))\n",
    "    print(\"Size of test set = %d\" % len(y_test))\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Data generator for fit and evaluate\n",
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, SEQUENCE_LEN), dtype=np.int32)\n",
    "        y = np.zeros((batch_size), dtype=np.int32)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "                x[i, t] = word_indices[w]\n",
    "            y[i] = word_indices[next_word_list[index % len(sentence_list)]]\n",
    "            index = index + 1\n",
    "        yield x, y\n",
    "\n",
    "\n",
    "def get_model(dropout=0.2):\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(words), output_dim=1024))\n",
    "    model.add(MultiplicativeLSTM(128))\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "#     model.add(Bidirectional(MultiplicativeLSTM(128)))\n",
    "#     if dropout > 0:\n",
    "#         model.add(Dropout(dropout))\n",
    "#     model.add(Bidirectional(MultiplicativeLSTM(128)))\n",
    "#     if dropout > 0:\n",
    "#         model.add(Dropout(dropout))\n",
    "    model.add(Dense(len(words)))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "\n",
    "    # Randomly pick a seed sequence\n",
    "    seed_index = np.random.randint(len(sentences+sentences_test))\n",
    "    seed = (sentences+sentences_test)[seed_index]\n",
    "\n",
    "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        sentence = seed\n",
    "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
    "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "        examples_file.write(' '.join(sentence))\n",
    "\n",
    "        for i in range(150):\n",
    "            x_pred = np.zeros((1, SEQUENCE_LEN))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x_pred[0, t] = word_indices[word]\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = indices_word[next_index]\n",
    "\n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word)\n",
    "\n",
    "            examples_file.write(\" \"+next_word)\n",
    "        examples_file.write('\\n')\n",
    "    examples_file.write('='*80 + '\\n')\n",
    "    examples_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('./checkpoints/'):\n",
    "    os.makedirs('./checkpoints/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for corpus in os.listdir(os.getcwd()+\"/aclImdb/train/pos\"):\n",
    "    try:\n",
    "        with io.open(os.getcwd()+\"/aclImdb/train/pos/\"+corpus, encoding='utf-8') as f:\n",
    "            text += re.sub(r'[^a-zA-Z0-9., ]', '', f.read().lower().replace('<br />', ' ').replace('.', ' . ').replace(',', ' , '))\n",
    "    except:\n",
    "        print(\"File: \"+os.getcwd()+\"/aclImdb/train/pos/\"+corpus+\" not found.\")\n",
    "\n",
    "for corpus in os.listdir(os.getcwd()+\"/aclImdb/test/pos\"):\n",
    "    try:\n",
    "        with io.open(os.getcwd()+\"/aclImdb/test/pos/\"+corpus, encoding='utf-8') as f:\n",
    "            text += re.sub(r'[^a-zA-Z0-9., ]', '', f.read().lower().replace('<br />', ' ').replace('.', ' . ').replace(',', ' , '))\n",
    "    except:\n",
    "        print(\"File: \"+os.getcwd()+\"/aclImdb/test/pos/\"+corpus+\" not found.\")\n",
    "\n",
    "with open('pos.txt','w') as f:\n",
    "    print(text, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for corpus in os.listdir(os.getcwd()+\"/aclImdb/train/neg\"):\n",
    "    try:\n",
    "        with io.open(os.getcwd()+\"/aclImdb/train/neg/\"+corpus, encoding='utf-8') as f:\n",
    "            text += re.sub(r'[^a-zA-Z0-9., ]', '', f.read().lower().replace('<br />', ' ').replace('.', ' . ').replace(',', ' , '))\n",
    "    except:\n",
    "        print(\"File: \"+os.getcwd()+\"/aclImdb/train/neg/\"+corpus+\" not found.\")\n",
    "\n",
    "for corpus in os.listdir(os.getcwd()+\"/aclImdb/test/neg\"):\n",
    "    try:\n",
    "        with io.open(os.getcwd()+\"/aclImdb/test/neg/\"+corpus, encoding='utf-8') as f:\n",
    "            text += re.sub(r'[^a-zA-Z0-9., ]', '', f.read().lower().replace('<br />', ' ').replace('.', ' . ').replace(',', ' , '))\n",
    "    except:\n",
    "        print(\"File: \"+os.getcwd()+\"/aclImdb/test/neg/\"+corpus+\" not found.\")\n",
    "\n",
    "with open('neg.txt','w') as f:\n",
    "    print(text, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in characters: 33425456\n",
      "Corpus length in words: 6353168\n",
      "Unique words before ignoring: 106044\n",
      "Ignoring words with frequency < 500\n",
      "Unique words after ignoring: 1133\n",
      "Ignored sequences: 6045424\n",
      "Remaining sequences: 307729\n",
      "Shuffling sentences\n",
      "Size of training set = 276956\n",
      "Size of test set = 30773\n",
      "Build model...\n",
      "Epoch 1/100\n",
      "8655/8655 [==============================] - 275s 32ms/step - loss: 4.4533 - acc: 0.1758 - val_loss: 4.1344 - val_acc: 0.2046\n",
      "Epoch 2/100\n",
      "8655/8655 [==============================] - 276s 32ms/step - loss: 4.0571 - acc: 0.2112 - val_loss: 4.0017 - val_acc: 0.2186\n",
      "Epoch 3/100\n",
      "8655/8655 [==============================] - 277s 32ms/step - loss: 3.9430 - acc: 0.2222 - val_loss: 3.9715 - val_acc: 0.2205\n",
      "Epoch 4/100\n",
      " 536/8655 [>.............................] - ETA: 4:16 - loss: 3.8871 - acc: 0.2298"
     ]
    }
   ],
   "source": [
    "# Positive training\n",
    "examples = \"results_pos.txt\"\n",
    "vocabulary = \"vocabulary_pos.txt\"\n",
    "\n",
    "\n",
    "text = \"\"\n",
    "with open('pos.txt','r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "print('Corpus length in characters:', len(text))\n",
    "\n",
    "text_in_words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "print('Corpus length in words:', len(text_in_words))\n",
    "\n",
    "# Calculate word frequency\n",
    "word_freq = {}\n",
    "for word in text_in_words:\n",
    "    word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "ignored_words = set()\n",
    "for k, v in word_freq.items():\n",
    "    if word_freq[k] < MIN_WORD_FREQUENCY:\n",
    "        ignored_words.add(k)\n",
    "\n",
    "words = set(text_in_words)\n",
    "print('Unique words before ignoring:', len(words))\n",
    "print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
    "words = sorted(set(words) - ignored_words)\n",
    "print('Unique words after ignoring:', len(words))\n",
    "\n",
    "with open(vocabulary, 'w') as f:\n",
    "    for item in words:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "indices_word = dict((i, c) for i, c in enumerate(words))\n",
    "\n",
    "# cut the text in semi-redundant sequences of SEQUENCE_LEN words\n",
    "sentences = []\n",
    "next_words = []\n",
    "ignored = 0\n",
    "for i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):\n",
    "    # Only add the sequences where no word is in ignored_words\n",
    "    if len(set(text_in_words[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
    "        sentences.append(text_in_words[i: i + SEQUENCE_LEN])\n",
    "        next_words.append(text_in_words[i + SEQUENCE_LEN])\n",
    "    else:\n",
    "        ignored = ignored + 1\n",
    "print('Ignored sequences:', ignored)\n",
    "print('Remaining sequences:', len(sentences))\n",
    "\n",
    "# x, y, x_test, y_test\n",
    "(sentences, next_words), (sentences_test, next_words_test) = shuffle_and_split_training_set(sentences, next_words, 10)\n",
    "\n",
    "model = get_model()\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "file_path=\"checkpoints/MLSTM-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True)\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=20)\n",
    "callbacks_list = [checkpoint, print_callback, early_stopping]\n",
    "\n",
    "examples_file = open(examples, \"w\")\n",
    "history = model.fit_generator(generator(sentences, next_words, BATCH_SIZE),\n",
    "                    steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "                    epochs=100,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),\n",
    "                    validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1e0f35ce6e64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# list all data in history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# summarize history for accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"data/train/neg\"\n",
    "examples = \"results_neg.txt\"\n",
    "vocabulary = \"vocabulary_neg.txt\"\n",
    "\n",
    "if not os.path.isdir('./checkpoints/'):\n",
    "    os.makedirs('./checkpoints/')\n",
    "\n",
    "text = \"\"\n",
    "for corpus in os.listdir(os.getcwd()):\n",
    "    with io.open(corpus, encoding='utf-8') as f:\n",
    "        text.append(f.read().lower().replace('<br />', ' ').replace('\\n', ' ').replace(u\"\\u201c\", \" \\\" \").replace(u\"\\u201d\", \" \\\" \").replace(u\"\\u2018\", \" ' \").replace(u\"\\u2019\", \" ' \").replace(\"--\", \"-\").replace(\"-\", \" - \").replace(\"?\", \" ? \").replace(\".\", \" . \").replace(\",\", \" , \").replace(\";\", \" ; \").replace(\"(\", \" ( \").replace(\")\", \" ) \").replace(\"_\", \" \").replace(\"[\", \" [ \").replace(\"]\", \" ] \").replace(\":\", \" : \").replace(\"!\", \" ! \"))\n",
    "#     text = re.sub(r'[^a-zA-Z0-9' ]', '', text)\n",
    "print('Corpus length in characters:', len(text))\n",
    "\n",
    "text_in_words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "print('Corpus length in words:', len(text_in_words))\n",
    "\n",
    "# Calculate word frequency\n",
    "word_freq = {}\n",
    "for word in text_in_words:\n",
    "    word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "ignored_words = set()\n",
    "for k, v in word_freq.items():\n",
    "    if word_freq[k] < MIN_WORD_FREQUENCY:\n",
    "        ignored_words.add(k)\n",
    "\n",
    "words = set(text_in_words)\n",
    "print('Unique words before ignoring:', len(words))\n",
    "print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
    "words = sorted(set(words) - ignored_words)\n",
    "print('Unique words after ignoring:', len(words))\n",
    "\n",
    "with open(vocabulary, 'w') as f:\n",
    "    for item in words:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "indices_word = dict((i, c) for i, c in enumerate(words))\n",
    "\n",
    "# cut the text in semi-redundant sequences of SEQUENCE_LEN words\n",
    "sentences = []\n",
    "next_words = []\n",
    "ignored = 0\n",
    "for i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):\n",
    "    # Only add the sequences where no word is in ignored_words\n",
    "    if len(set(text_in_words[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
    "        sentences.append(text_in_words[i: i + SEQUENCE_LEN])\n",
    "        next_words.append(text_in_words[i + SEQUENCE_LEN])\n",
    "    else:\n",
    "        ignored = ignored + 1\n",
    "print('Ignored sequences:', ignored)\n",
    "print('Remaining sequences:', len(sentences))\n",
    "\n",
    "# x, y, x_test, y_test\n",
    "(sentences, next_words), (sentences_test, next_words_test) = shuffle_and_split_training_set(\n",
    "    sentences, next_words\n",
    ")\n",
    "\n",
    "model = get_model()\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "# file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-\" \\\n",
    "#             \"loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}\" % \\\n",
    "\n",
    "file_path=\"checkpoints/MLSTM-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True)\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=20)\n",
    "callbacks_list = [checkpoint, print_callback, early_stopping]\n",
    "\n",
    "examples_file = open(examples, \"w\")\n",
    "history = model.fit_generator(generator(sentences, next_words, BATCH_SIZE),\n",
    "                    steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "                    epochs=100,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),\n",
    "                    validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
